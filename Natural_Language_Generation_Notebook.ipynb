{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honors College IHT Capstone | Example-Based Natural Language Generation with Context-Free Phrase Structure Grammars to Analyze Poverty of Stimulus Arguments\n",
    "#### Author: Brian Brown | May 2020 | brianbrown0417@gmail.com\n",
    "\n",
    "### Motivation\n",
    "I was motivated to start this project after reading The Language Instinct by Steven Pinker. I was fascinated by the idea that humans possess a biological language faculty as a product of evolution. \n",
    "\n",
    "In my undergraduate coursework, I looked deeper into surrounding theories, such as the argument from the poverty of stimulus. In short, this is the idea that children are not exposed to rich enough data within their linguistic environments to acquire every feature of their language; therefore, they must posses some knowledge innately.\n",
    "\n",
    "I also researched compilers, relying heavily on context-free grammars and different parsing algorithms (LL1, CKY, etc.). This knowledge paired well with my existing linguistic knowledge. However, I wanted to get serious, so I read Syntactic Structures by Noam Chomsky. This helped me understand how grammars could be represented in phrase structures, and how simple transformations create the possibility for an almost infinite number of possibilities.\n",
    "\n",
    "### Goals\n",
    "The aim of this notebook is to explore some of the fundamental linguistic tools and how they can be served to construct a simple, albeit effective natural langauge processing system. There were a lot of useful references, but I found https://www.thoughtco.com/phrase-structure-grammar-1691509 to be especially useful.\n",
    "\n",
    "The first fundamental aspect is how context free-grammars can be used to describe consituent rules in English. On top of that, feature structures to ensure grammatical and semantic concord can be utilize to augment the context free grammar. Finally, some simple logic can go a long way in allowing the system to perform specific tasks.\n",
    "\n",
    "The uasage of formal grammar rules show the potential of phrase structure grammars. In addition, the results can help inform our view of the poverty of stimulus argument. Showing the capabilities of a system with this strucutre, and the sentences it is not only able to parse, but also generate, is a testament to the robustness of the poverty of stimulus argument. This notebook does not rely on rigorous training, which offers an interesting tradoff. While this is a high maintenace solution. It is highly structured, modular, and permits iterability. These new iterations can be viewed as simulating language acquisition from stimulus. \n",
    "\n",
    "Nonetheless, the debate surrounding the poverty of stimulus are broad and contreversial. While the results of this notebook barely scratch the surface of, it is enough to provoke thought on the topic, and gain a unique, functional persepctive through computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Constructing the Grammar\n",
    "Part 1 aims to construct a grammatical structure that represents the English Language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Grammar Class\n",
    "\n",
    "The following class is responsible for holding all structures associated with the grammar. It also includes some other simple object oriented functions to facilitate object-oriented functionality. This can be thought of as the cornerstone of this entire notebook. It holds the very basic rules that make up the grammar, and it does so with a very simple list of of rules.\n",
    "\n",
    "I should also note that nltk proved to be very useful here. I will talk more about the feature stucture library; however, the functionality that is available after one import statement is astonishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "class Grammar(object):\n",
    "    \n",
    "    #basic constructure that initializes an empty dictionary to store rules\n",
    "    def __init__(self):      \n",
    "        self.rules = []\n",
    "        \n",
    "        \n",
    "    def add_rule(self, feature_structure):\n",
    "        \n",
    "        for i in self.rules:\n",
    "            if i == feature_structure:\n",
    "                return\n",
    "            \n",
    "        self.rules.append(feature_structure)     \n",
    "    \n",
    "    \n",
    "    def apply_rules(self, value):\n",
    "        \n",
    "        tags = value.split(' ')\n",
    "        \n",
    "        if len(tags) == 1:\n",
    "            \n",
    "            for i in self.rules:   \n",
    "                if i['parent']['terminal'] == 'true' and i['child1']['tag'] == tags[0]:  \n",
    "                    return i['parent']\n",
    "                \n",
    "        elif len(tags) == 2:\n",
    "                        \n",
    "            for i in self.rules:\n",
    "                if i['parent']['terminal'] == 'false' and i['child1']['tag'] == tags[0] and i['child2']['tag'] == tags[1]: \n",
    "                    return i['parent']\n",
    "        else:\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def get_rules(self, key):\n",
    "        \n",
    "        feature_structure_list = []\n",
    "        \n",
    "        for i in self.rules:\n",
    "            if i['parent']['tag'] == key:\n",
    "                feature_structure_list.append(i)\n",
    "                \n",
    "        return feature_structure_list\n",
    "    \n",
    "    \n",
    "    def print_expansion(self, production):\n",
    "        if production.get_left() is None or production.get_right() is None:\n",
    "            return production.get_text()\n",
    "        else:\n",
    "            return '[' + str(production.get_left().get_head()['tag']) + ' ' + self.print_expansion(production.get_left()) + '][' + str(production.get_right().get_head()['tag']) + ' ' + self.print_expansion(production.get_right()) + ']'\n",
    "       \n",
    "    def check_duplicate(self, list, string): #this function assumes there is no ambiguity in the grammar, and that a string can only correspond to a set of rules\n",
    "        for i in list:\n",
    "            if i[0].get_text() == string:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def print_rules(self):\n",
    "        for r in self.rules:\n",
    "            print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Before going any further, I thought it was important to make sure that the functions contained in the grammar class were not going to fail. First, we can instantiate a basic grammar object, and pass some simple rules. The tags here are modified from this treebank: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "This is the first look at how a rule is structured within the nltk featStruct library. We go into more CFG details below, but now it's useful to analyze the tags contained within the parent, and its children.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg = Grammar()\n",
    "test_cfg.add_rule(nltk.FeatStruct('[parent=[tag=S, terminal=false], child1=[tag=NP], child2=[tag=VP]]'))\n",
    "test_cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=false], child1=[tag=DT], child2=[tag=NOM]]'))\n",
    "test_cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=true], child1=[tag=dog]]'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first important function is the find the LHS structure, or parent given a RHS. This is accomplished through the apply_rules() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ tag      = 'NOM'  ]\n",
      "[ terminal = 'true' ]\n"
     ]
    }
   ],
   "source": [
    "print(test_cfg.apply_rules('dog'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ tag      = 'S'     ]\n",
      "[ terminal = 'false' ]\n"
     ]
    }
   ],
   "source": [
    "print(test_cfg.apply_rules('NP VP'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These both work as they are intended. The first recognizes that the word 'dog' in a nominal, and the second is able to work backwards and recognize the rule S -> NP VP. \n",
    "\n",
    "It is also going to be useful to work in the opposite direction. We may use a bottom-up approach to parse, but then later use a top down appoach for something else. By coding both of these functions from the start (and testing them), it will save time later on. To find the accompanying rule given a LHS string, we use the get_rules() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[child1=[tag='NP'], child2=[tag='VP'], parent=[tag='S', terminal='false']]]\n"
     ]
    }
   ],
   "source": [
    "print(test_cfg.get_rules('S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[child1=[tag='dog'], parent=[tag='NOM', terminal='true']]]\n"
     ]
    }
   ],
   "source": [
    "print(test_cfg.get_rules('NOM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tests pass. The next step will be building out the grammar to more closely resemble English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context-Free Grammars\n",
    "A context-free grammars serve as a set of rules to construct patterns of strings. Their rules are soemetimes called productions, and they recursively describe how symbols can be expanded into new patterns. A context-free grammar contains:\n",
    "- A group of terminal symbols, which are the characters of the alphabet that appear in the generataed strings\n",
    "- A group of non-terminal symbols, which can be thought of as placeholders for patterns of terminal symbols \n",
    "- A set of productions, which are the rules of the grammar. They contain a left-hand-side and a right-hand side, and specify which symbols are included in the production\n",
    "- A start symbol, which is the first non-terminal symbol from which strings are generated\n",
    "\n",
    "For the purposes of this project, a CFG will be used to describe (in broad strokes) natural English Language. The grammar rules can be likened to the foundation of a theoretical Universal Grammar. To understand the fundamentals of CFGs a useful, I found this to be a useful document: https://www.cs.rochester.edu/~nelson/courses/csc_173/grammars/cfg.html.\n",
    "\n",
    "A special form of CFGs is called Chomsky normal form. A CFG is in Chomsky normal form where the rules satisfy the one of the following conditions:\n",
    "- A -> BC where A, B, and C are non-terminals and B and C are not the start terminal\n",
    "- A -> a where A is non-terminal and a is terminal\n",
    "\n",
    "Within this structure, the rules are stored with the feature structures unites [parent] -> [child1] [child2]\n",
    "\n",
    "This structure is advantageous for both sentence generation and parsing. It allows us to structure the grammar as a binary tree, and exploit it for both its efficiency and simplicity. Here's another good source: https://courses.cs.washington.edu/courses/cse322/08au/lec14.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.A Reflection:\n",
    "The main takeaway from this part would have been to conduct all of my planning and research prior to beginning. I found myself constantly backtracking on redundant code, or trying to fix the inadequacies in my grammar. The only important point was to practice good habits with writing object oriented code. This will be more pertinent a little bit later on, but doing so saves a lot of time when trying to access information. Rarely, even in school projects, do you construct a code base this large without any instruction. It was a great exercise in utilizing best practices in coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Feature Structures\n",
    "\n",
    "While CFGs are powerful, they cannot capture all of the grammatical information that we care about. A more elegant approach harnesses the power of feature structures. But what are feature structures?\n",
    "\n",
    "Feature structures are sets of attribute/value pairs. They can hold non-grammatical inforamtion; however, they are commonly used to form agreement rules within a grammar. A feature structure can be represented as a directed acyclic graph (DAG), with the nodes corresponding to the variable values and the paths to the variable names.\n",
    "\n",
    "Feature structures can be unified to combine the attribute/value pairs of two linguistic units. To get a better understanding of this, I recommend reading the nltk guide. It provides a lot of use case examples: https://www.nltk.org/book/ch09.html\n",
    "\n",
    "There's a lot of agreement rules in English, and other languages have their own unique rules. However, a lot of these are fringe cases. A good example is how a language like relies heavily on gender - while English only relies on gender agreement for a few uses. The most notable is pronouns (SHE threw HER bag in the trunk). Take a step back from that and you'll see that pronoun's themselves present a difficult challenge for computers since tracing back their antecedent requires a complete understanding of semantic context.\n",
    "\n",
    "With all of this in mind, I decided to focus on two important forms of agreement: Number and Tense. These were easily formalized and there weren't too many exceptions.\n",
    "\n",
    "#### Number Agreement\n",
    "Number agreement is concord in form (as singular, dual, or plural) between determiner-noun, pronoun-antecedent, and noun-verb. We will be using an NLTK feature structure grammar to declare these rules. Upone combining rules during generation or parsing, we will use the NLTK library to parse the new production, and see if it complies with the grammar.\n",
    "\n",
    "#### Tense Consistency\n",
    "Tense consistency is concord of verb tense across a clause. The simple forms are past, present, and future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Rules\n",
    "Here, the rules are defined that describe English. This may not look like much, but it includes all of the basic productions needed to produce English sentences, as well as the agreement rules that keep them together. Look at the comments for more details on each individual rule.\n",
    "\n",
    "Below, we populate the terminal objects. The number and tense values are included in the agr feature structure. Eventually, we'll make direct comparisons between these structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Grammar()\n",
    "\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=S, terminal=false, agr=[num=?n]], child1=[tag=NP, agr=[num=?n]], child2=[tag=VP, agr=[num=?n]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=false, agr=[num=?n]], child1=[tag=DT, agr=[num=?n]], child2=[tag=NOM, agr=[num=?n]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=false, agr=[num=?n]], child1=[tag=DT, agr=[num=?n]], child2=[tag=NP, agr=[num=?n]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=false, agr=[num=pl]], child1=[tag=NP, agr=[]], child2=[tag=CCNP, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=false, agr=[num=pl]], child1=[tag=NOM, agr=[]], child2=[tag=NOM, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=false, agr=[num=?n]], child1=[tag=POS, agr=[]], child2=[tag=NOM, agr=[num=?n]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=false, agr=[num=?n]], child1=[tag=JJR, agr=[]], child2=[tag=NOM, agr=[num=?n]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=CCNP, terminal=false, agr=[]], child1=[tag=CC, agr=[]], child2=[tag=NOM, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=PP, terminal=false, agr=[]], child1=[tag=IN, agr=[]], child2=[tag=NP, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=VB, agr=[num=?n, tense=?t]], child2=[tag=NP, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=VB, agr=[num=?n, tense=?t]], child2=[tag=PP, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=VP, agr=[num=?n, tense=?t]], child2=[tag=PP, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[]], child1=[tag=MD, agr=[]], child2=[tag=VP, agr=[tense=?t]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=AUX, agr=[tense=?t]], child2=[tag=VB, agr=[tense=?t]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=AUX, agr=[tense=?t]], child2=[tag=VP, agr=[tense=?t]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=VP, agr=[tense=?t]], child2=[tag=CCVP, agr=[tense=?t]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=VB, agr=[num=?n, tense=?t]], child2=[tag=RB, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VP, terminal=false, agr=[num=?n, tense=?t]], child1=[tag=VP, agr=[num=?n, tense=?t]], child2=[tag=RB, agr=[]]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=CCVP, terminal=false, agr=[tense=?t]], child1=[tag=CC, agr=[]], child2=[tag=VP, agr=[tense=?t]]]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominals, Pronouns, and Proper Nouns\n",
    "For these, number agreement is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=true, agr=[num=sg]], child1=[tag=dog]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=true, agr=[num=sg]], child1=[tag=flight]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=true, agr=[num=pl]], child1=[tag=planes]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=true, agr=[num=sg]], child1=[tag=takeoff]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=true, agr=[num=sg]], child1=[tag=airport]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NOM, terminal=true, agr=[num=pl]], child1=[tag=children]]'))\n",
    "\n",
    "#Proper Nouns and Pronouns are mapped directly to NP\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=true, agr=[num=sg]], child1=[tag=I]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=true, agr=[num=sg]], child1=[tag=me]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=true, agr=[num=sg]], child1=[tag=Alaska]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=true, agr=[num=sg]], child1=[tag=Seattle]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=true, agr=[num=sg]], child1=[tag=Brian]]'))\n",
    "\n",
    "\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=NP, terminal=true, agr=[num=sg]], child1=[tag=I]]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs, Verb Phrases, Modals, and Auxillaries\n",
    "Verbs must account for number agreement, as well as tense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=pl, tense=pres], subcat=[empty=n, first=PP, rest=[empty=y]]], child1=[tag=fly]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=pl, tense=pres]], child1=[tag=go]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=pl, tense=pres]], child1=[tag=take]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=sg, tense=pres]], child1=[tag=leaves]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=pl, tense=pres]], child1=[tag=hate]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=sg, tense=pres]], child1=[tag=is]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=pl, tense=pres]], child1=[tag=are]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=pl, tense=pres]], child1=[tag=shake]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=?n, tense=past]], child1=[tag=found]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=pl, tense=pres]], child1=[tag=ride]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[num=?n, tense=past]], child1=[tag=told]]'))\n",
    "\n",
    "#AUX VERB - the production should only take the tense value of the auxillary verb\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=AUX, terminal=true, agr=[tense=pres]], child1=[tag=does]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=AUX, terminal=true, agr=[tense=past]], child1=[tag=have]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=AUX, terminal=true, agr=[tense=future]], child1=[tag=will]]'))\n",
    "\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=MD, terminal=true, agr=[tense=?t]], child1=[tag=can]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=MD, terminal=true, agr=[tense=?t]], child1=[tag=might]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=MD, terminal=true, agr=[tense=?t]], child1=[tag=could]]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determiners and Conjunctions\n",
    "Like nouns, determiners are primarily concerned with number agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=DT, terminal=true, agr=[num=?n]], child1=[tag=the]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=DT, terminal=true,agr=[num=sg]], child1=[tag=a]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=DT, terminal=true, agr=[num=pl]], child1=[tag=those]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=DT, terminal=true, agr=[num=sg]], child1=[tag=that]]'))\n",
    "\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=CC, terminal=true, agr=[]], child1=[tag=and]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=CC, terminal=true, agr=[]], child1=[tag=but]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=CC, terminal=true, agr=[]], child1=[tag=yet]]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepositions, Adjectives, Adverbs, and Possessives\n",
    "The following inhertic features from the noun or verb they are pairing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=IN, terminal=true, agr=[]], child1=[tag=from]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=IN, terminal=true, agr=[]], child1=[tag=up]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=IN, terminal=true, agr=[]], child1=[tag=off]]'))\n",
    "\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=RB, terminal=true, agr=[]], child1=[tag=quickly]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=RB, terminal=true, agr=[]], child1=[tag=wildly]]'))\n",
    "\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=JJR, terminal=true, agr=[]], child1=[tag=long]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=JJR, terminal=true, agr=[]], child1=[tag=tired]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=JJR, terminal=true, agr=[]], child1=[tag=big]]'))\n",
    "\n",
    "#Superlative Adjectives\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=JJS, terminal=true, agr=[]], child1=[tag=longest]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=JJS, terminal=true, agr=[]], child1=[tag=worst]]'))\n",
    "\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=POS, terminal=true, agr=[]], child1=[tag=my]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=POS, terminal=true, agr=[]], child1=[tag=his]]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Expansion Testing\n",
    "\n",
    "We have a random expansion function to make sure that we are able to generate sentences that are somewhat grammatrically sound. The flatten() function is used to clean up the output. The output has tendency to get distored since append() is used in conjunction iwht many recursive calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(items, seqtypes=(list, tuple)):\n",
    "    for i, x in enumerate(items):\n",
    "        while i < len(items) and isinstance(items[i], seqtypes):\n",
    "            items[i:i+1] = items[i]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "expanding S:\n",
      "----\n",
      "expanding NP:\n",
      "----\n",
      "expanding DT:\n",
      "found terminal node: those\n",
      "----\n",
      "expanding NOM:\n",
      "----\n",
      "expanding JJR:\n",
      "found terminal node: tired\n",
      "----\n",
      "expanding NOM:\n",
      "----\n",
      "expanding JJR:\n",
      "found terminal node: tired\n",
      "----\n",
      "expanding NOM:\n",
      "found terminal node: airport\n",
      "----\n",
      "expanding VP:\n",
      "----\n",
      "expanding AUX:\n",
      "found terminal node: does\n",
      "----\n",
      "expanding VP:\n",
      "----\n",
      "expanding AUX:\n",
      "found terminal node: does\n",
      "----\n",
      "expanding VB:\n",
      "found terminal node: shake\n",
      "\n",
      "those tired tired airport does does shake\n"
     ]
    }
   ],
   "source": [
    "def expand_random(grammar, rule):\n",
    "        \n",
    "    print('----')\n",
    "    print('expanding ' + rule + ':')\n",
    "        \n",
    "    sentence = []\n",
    " \n",
    "    random_rule = random.choice(grammar.get_rules(rule))\n",
    "            \n",
    "    #probably can take out the is_terminal function\n",
    "    if random_rule['parent']['terminal'] == 'true':\n",
    "        print('found terminal node: ' + random_rule['child1']['tag'])\n",
    "        return random_rule['child1']['tag']\n",
    "                        \n",
    "    if random_rule.has_key('child1'):\n",
    "        child1_tag = random_rule['child1']['tag']\n",
    "        sentence.append(expand_random(grammar, child1_tag))\n",
    "            \n",
    "    if random_rule.has_key('child2'):\n",
    "        child2_tag = random_rule['child2']['tag']\n",
    "        sentence.append(expand_random(grammar, child2_tag))\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "sentence = flatten(expand_random(cfg, 'S'))\n",
    "print('\\n'+' '.join([str(elem) for elem in sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is gramatically sound according to the cfg, but is otherwise meaningless. It's still a fun and useful exercise to see what's possible from a few simple grammar rules and a top-down generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Unification:\n",
    "Another thing to note about the random expansions above is that they lack any sort of agreement. In order to utilize the agreement structures in our grammar class, we need to harness the power of feature unification. Feature unification seeks to combine two nodes by looking at certain attribute value pairs. If the values match, then the matching value will be returned. Otherwise, it will return None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Classes\n",
    "The following helper classes will be used in future tasks like parsing and phrase generation. We are going to declare them now to ensure that our agreement functions are compatible with those tasks. The following class is called Production. It serves as a richer linguistic unit that stores not only the part of speech, but pointers to children, the complete text of the linguistic unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Production(object):\n",
    "    \n",
    "    head = None\n",
    "    left = None\n",
    "    right = None\n",
    "    text = ''\n",
    "    \n",
    "    # Constructor which takes in the head of the rule, and two child rules\n",
    "    def __init__(self, head, left, right, text):\n",
    "        self.head = head\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.text = text\n",
    "    \n",
    "    def get_head(self):\n",
    "        return self.head\n",
    "    \n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "    \n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    def get_text(self):\n",
    "        return self.text\n",
    "    \n",
    "    def get_attr(self):\n",
    "        return self.attribute_label\n",
    "    \n",
    "#declare Cell later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unification Function\n",
    "The following unification function analyzes two children that the system is trying to merge. The first step is to understand the rule that is being used to merge the two, and obtain its parent. From there, the function will attempt to unify the 'agr' structure that is nested within each of the children, and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unification(child1, child2, grammar):\n",
    "            \n",
    "    child1_fs_agr = child1.get_head()['agr']\n",
    "    child2_fs_agr = child2.get_head()['agr']\n",
    "\n",
    "    parent_fs = grammar.apply_rules(child1.get_head()['tag'] + ' ' + child2.get_head()['tag'])\n",
    "    parent_fs_agr = parent_fs['agr']\n",
    "    \n",
    "    if child1_fs_agr is None or child2_fs_agr is None or parent_fs_agr is None:\n",
    "        return None\n",
    "\n",
    "    if parent_fs_agr.has_key('num'):\n",
    "        \n",
    "        if parent_fs_agr['num'] == 'sg' or parent_fs_agr['num'] == 'pl':\n",
    "            \n",
    "            child1_fs_agr['num'] = parent_fs_agr['num']\n",
    "            child2_fs_agr['num'] = parent_fs_agr['num']\n",
    "        \n",
    "        try:\n",
    "            return parent_fs_agr.unify(child1_fs_agr).unify(child2_fs_agr)\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    elif parent_fs_agr.has_key('tense'):\n",
    "        \n",
    "        if child1.get_head()['tag'] == 'AUX' and child2_fs_agr.has_key('tense'):\n",
    "            \n",
    "            child2_fs_agr.pop('tense')\n",
    "            \n",
    "        try:\n",
    "            return parent_fs_agr.unify(child1_fs_agr).unify(child2_fs_agr)\n",
    "        except:\n",
    "            return None    \n",
    "\n",
    "    else:\n",
    "        \n",
    "        return parent_fs_agr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "The goal here is to test the capabilities of number agreement, consistency, and the assumption of values when featured nodes are paired with unfeatured nodes (like an comparative adjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ num = 'sg' ]\n"
     ]
    }
   ],
   "source": [
    "production_1 = Production(cfg.apply_rules('big'),None,None,'big')\n",
    "production_2 = Production(cfg.apply_rules('dog'),None,None,'dog')\n",
    "print(unification(production_1,production_2,cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ num = 'pl' ]\n"
     ]
    }
   ],
   "source": [
    "production_1 = Production(cfg.apply_rules('the'),None,None,'the')\n",
    "production_2 = Production(cfg.apply_rules('children'),None,None,'children')\n",
    "print(unification(production_1,production_2,cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "production_1 = Production(cfg.apply_rules('that'),None,None,'that')\n",
    "production_2 = Production(cfg.apply_rules('children'),None,None,'children')\n",
    "print(unification(production_1,production_2,cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n"
     ]
    }
   ],
   "source": [
    "production_1 = Production(cfg.apply_rules('fly'),None,None,'fly')\n",
    "production_2 = Production(cfg.apply_rules('quickly'),None,None,'quickly')\n",
    "print(unification(production_1,production_2,cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aboce examples showcase a few different outcomes. These can be either that the value is inferred from one node (such as Noun Phrase -> Nominal Adjective), the value is successfully unified from two objects (Noun Phrase -> Determiner Nominal), or the the values do not match (None is returned)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.B Reflection:\n",
    "\n",
    "The was a lot to upack here, but something that I learned was with my code sometimes simpler was better. I learned that if i provided enough information in the actual grammar, that would make feature unification much simpler. \n",
    "\n",
    "The other takeaway, whichwas on the linguistic end was that I didn't always think about rules and agreement. They were things that I took for granted, and prior to this project, I actually would have had trouble articulating rules. That goes to show that when you are trying to formally encode information, you have to challenge assumptions and really do your homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C: Verb Arguments\n",
    "Another important part of our feature grammar is the ability to understand verb arguments. Verb argument structure is a complement to phrase structure grammar. It suggests that verbs belong to different classes according what events the verbs reference.\n",
    "\n",
    "Events are occurances of the verb, and there are certain roles that play a part. These roles can be simple nouns serving as a receiving object, or more complex phrases. These supporting roles are sometimes called theta-roles. There are usually betweeen 0 and 3. For simplicity, we will look at the following classes of verbs:\n",
    "\n",
    "#### Transitive:\n",
    "Transitive verbs are characterized by the obligatory presence of two non-prepositional arguments: a\n",
    "subject and a direct object. Transitive verbs assign accusative case to their direct object, while the\n",
    "subject receives nominative case from the [+finite] specification of the Tense in the clause.\n",
    "\n",
    "- John[AGENT] broke the window[PATIENT]\n",
    "- John[AGENT] painted the door[THEME]\n",
    "- John[AGENT] made dinner[FACITIVE]\n",
    "- The wind[CAUSE] moves the grass[THEME]\n",
    "\n",
    "#### DIntransitive:\n",
    "Ditransitive verbs are very similar to transitive verbs but they have one more argument, which is\n",
    "traditionally called indirect object. In both Italian and English it can be realized with a PP headed by\n",
    "the preposition to/a. \n",
    "\n",
    "- John[ORIGIN] gave (a book)[THEME] (to Mary)[GOAL/RECEIVER]\n",
    "- John sent (a letter)[THEME] (to New York)[GOAL/RECEIVER]\n",
    "\n",
    "#### Intransitive:\n",
    "There are verbs that take no argument. Among these verbs we find weather vebs, predicates that describe a situation such as the adjectives.\n",
    "\n",
    "- It rains/ snows/ hails.\n",
    "- It’s me. \n",
    "\n",
    "A good place for more information on this is: https://www.unive.it/media/allegato/download/Lingue/Materiale%20didattico%20Giusti/Lingua_Inglese1/Argument_structure1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Feature Structure Represent This information\n",
    "Our feature structures will include a sucategory field. Since the number of arguments is variable, we need to get creative with how we represent a list. This will look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[          [ empty = 'n'                         ] ]\n",
      "[          [ first = 'np'                        ] ]\n",
      "[          [                                     ] ]\n",
      "[ subcat = [         [ empty = 'n'             ] ] ]\n",
      "[          [ rest  = [ first = 'pp'            ] ] ]\n",
      "[          [         [                         ] ] ]\n",
      "[          [         [ rest  = [ empty = 'y' ] ] ] ]\n"
     ]
    }
   ],
   "source": [
    "ditrans = nltk.FeatStruct('[subcat=[empty=n, first=np, rest=[empty=n, first=pp, rest=[empty=y]]]]')\n",
    "print(ditrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we populate a few more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transitive\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[tense=past], subcat=[empty=n, first=NP, rest=[empty=y]]], child1=[tag=broke]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[tense=pres], subcat=[empty=n, first=PP, rest=[empty=y]]], child1=[tag=look]]'))\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[tense=past], subcat=[empty=n, first=PP, rest=[empty=y]]], child1=[tag=made]]'))\n",
    "\n",
    "#ditransitive\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[tense=past], subcat=[empty=n, first=NP, rest=[empty=n, first=NP, rest=[empty=y]]]], child1=[tag=gave]]'))\n",
    "\n",
    "#intransitive\n",
    "cfg.add_rule(nltk.FeatStruct('[parent=[tag=VB, terminal=true, agr=[tense=pres], subcat=[empty=n, first=NP, rest=[empty=n, first=PP, rest=[empty=y]]]], child1=[tag=rains]]'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should only get called if the first argument is a vb or vp and has a subcat field - returns the new subcat fs\n",
    "def verb_argument(child1, child2, grammar):\n",
    "            \n",
    "    verb_subcat = child1.get_head()['subcat']\n",
    "    object_type = child2.get_head()['tag']\n",
    "    \n",
    "    if verb_subcat['empty'] == 'y':\n",
    "        \n",
    "        if object_type == 'NOM' or object_type == 'NP' or object_type == 'PP':\n",
    "            return None\n",
    "        else:\n",
    "            return verb_subcat\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if verb_subcat['first'] == object_type:\n",
    "            return verb_subcat['rest']\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Demo\n",
    "In this simple example: the verb is gave, which is ditransitive. it is expected NP for the first argument. However, the unification fails since it is passed an adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ empty = 'n'                         ]\n",
      "[ first = 'NP'                        ]\n",
      "[                                     ]\n",
      "[         [ empty = 'n'             ] ]\n",
      "[ rest  = [ first = 'NP'            ] ]\n",
      "[         [                         ] ]\n",
      "[         [ rest  = [ empty = 'y' ] ] ]\n",
      "----\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "production_1 = Production(cfg.apply_rules('gave'),None,None,'gave')\n",
    "production_2 = Production(cfg.apply_rules('wildly'),None,None,'wildly')\n",
    "\n",
    "print(production_1.get_head()['subcat'])\n",
    "print('----')\n",
    "print(verb_argument(production_1, production_2, cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, if the verb is given a NP - in this case a proper noun - it returns the subcat feature structure associated with the resulting unification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ empty = 'n'                         ]\n",
      "[ first = 'NP'                        ]\n",
      "[                                     ]\n",
      "[         [ empty = 'n'             ] ]\n",
      "[ rest  = [ first = 'NP'            ] ]\n",
      "[         [                         ] ]\n",
      "[         [ rest  = [ empty = 'y' ] ] ]\n",
      "----\n",
      "[ empty = 'n'             ]\n",
      "[ first = 'NP'            ]\n",
      "[                         ]\n",
      "[ rest  = [ empty = 'y' ] ]\n"
     ]
    }
   ],
   "source": [
    "production_1 = Production(cfg.apply_rules('gave'),None,None,'gave')\n",
    "production_2 = Production(cfg.apply_rules('Brian'),None,None,'Brian')\n",
    "\n",
    "print(production_1.get_head()['subcat'])\n",
    "print('----')\n",
    "print(verb_argument(production_1, production_2, cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.C Reflection:\n",
    "\n",
    "I think the biggest takeaway from part c was just more carryover from part 2. Looking at predicate argument structure showed me that there are sometimes multiple solution and clever workarounds when working with both linguistics and computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CFG Parsing\n",
    "In order to round out the program, we are going to test the CFG rules in a reverse manner: by parsing setences using the CYK parsing algorithm (Cocke–Younger–Kasami). This algorithm is designed for use specifically with CFGs and shows if sentences are accepted by the grammar. It uses a bottom-up appraoch with dynamic programming.\n",
    "\n",
    "Here is a very useful tool to visualize the parse table: https://www.xarg.org/tools/cyk-algorithm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting Objects Structures\n",
    "We need some addition helper objects to aid in parsing (and eventually search-based sentence generation). Notably, As the setence parses, there is some expected ambiguity. To account for this, we need to create a cell object that holds the multiple production rules that are an option for a given token or set of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(object):\n",
    "    \n",
    "    #list of production rules\n",
    "    productions = []\n",
    "    \n",
    "    #basic constructor with an optional parameter productions\n",
    "    def __init__(self, productions=None):\n",
    "        if productions is None:\n",
    "            self.productions = []\n",
    "        else:\n",
    "            self.productions = productions\n",
    "            \n",
    "    def add_production(self, result, p1, p2, text):\n",
    "        new_production = Production(result, p1, p2, text)\n",
    "        self.productions.append(new_production)\n",
    "    \n",
    "    def set_productions(self, p):\n",
    "        self.productions = p\n",
    "    \n",
    "    def get_results(self):\n",
    "        results = []\n",
    "        for p in self.productions:\n",
    "            results.append(p.get_head()['tag'])\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def get_rules(self):       \n",
    "        return self.productions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing\n",
    "The CYK algorithm parses using a dynamic table to help the possible derivations, and constructing a viable tree from the bottom up. We can implement this with a class that stores cell objects. Within the class, we can also put our parse function, that parses an input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parse_Table(object):\n",
    "    \n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.parse_table = None\n",
    "        self.length = 0\n",
    "    \n",
    "    #Print the CYK parse trable for the last sentence that have been parsed - this requires the tabulate library\n",
    "    def print_parse_table(self):      \n",
    "        lines = [] \n",
    "        \n",
    "        for row in reversed(self.parse_table):\n",
    "            l = []\n",
    "            for cell in row:\n",
    "                l.append(cell.get_results())\n",
    "            lines.append(l)\n",
    "        \n",
    "        lines.append(self.tokens)\n",
    "        print(tabulate(lines))\n",
    "        \n",
    "    #Returns a list containing the parent of the possible trees that we can generate for the last sentence that have been parsed\n",
    "    def get_trees(self):\n",
    "        return self.parse_table[self.length-1][0].productions\n",
    "    \n",
    "    #Parse a sentence (string) with the CYK algorithm   \n",
    "    def parse(self, sentence):\n",
    "        self.number_of_trees = 0\n",
    "        self.tokens = sentence.split()\n",
    "        self.length = len(self.tokens)\n",
    "        \n",
    "        if self.length < 1:\n",
    "            print('Error: unable to parse')\n",
    "            return\n",
    "        \n",
    "        #list comprehension to declare an empty parse table\n",
    "        self.parse_table = [ [Cell() for x in range(self.length - y)] for y in range(self.length) ]\n",
    "        \n",
    "        \n",
    "        for x, t in enumerate(self.tokens):\n",
    "            \n",
    "            r = self.grammar.apply_rules(t)\n",
    "            \n",
    "            if r == None:\n",
    "                raise ValueError(\"The word \" + str(t) + \" is not in the grammar\")\n",
    "            else:\n",
    "                self.parse_table[0][x].add_production(r, None, None, str(t))        \n",
    "                \n",
    "        \n",
    "        for l in range(2,self.length+1):\n",
    "            for s in range(1,self.length-l+2):\n",
    "                for p in range(1,l-1+1):\n",
    "                    \n",
    "                    t1 = self.parse_table[p-1][s-1].get_rules()\n",
    "                    t2 = self.parse_table[l-p-1][s+p-1].get_rules()\n",
    "                            \n",
    "                    for a in t1:\n",
    "                        for b in t2:\n",
    "                            \n",
    "                            r = self.grammar.apply_rules(str(a.get_head()['tag']) + \" \" + str(b.get_head()['tag']))\n",
    "                            \n",
    "                            if r is not None:\n",
    "                                \n",
    "                                if (a.get_head()['tag'] == 'VP' or a.get_head()['tag'] == 'VB') and a.get_head().has_key('subcat'):\n",
    "                                    return_subcat = verb_argument(a, b, self.grammar)\n",
    "                                    if return_subcat is None:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        print('--------')\n",
    "                                        print('Unified verb argument: ['+str(a.get_head()['tag'])+' '+a.get_text()+'] ['+str(b.get_head()['tag'])+' '+b.get_text()+']')\n",
    "                                        print(return_subcat)\n",
    "                                        print('--------')\n",
    "                                        r['subcat'] = return_subcat\n",
    "                    \n",
    "                                return_agr = unification(a,b,self.grammar)\n",
    "                                if return_agr is None:\n",
    "                                    print(str(a.get_head()['agr']))\n",
    "                                    print(str(b.get_head()['agr']))\n",
    "                                    print('Error number/tense agreement: ['+str(a.get_head()['tag'])+' '+a.get_text()+'] ['+str(b.get_head()['tag'])+' '+b.get_text()+']')\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    print('--------')\n",
    "                                    print('Unified number/tense agreement: ['+str(a.get_head()['tag'])+' '+a.get_text()+'] ['+str(b.get_head()['tag'])+' '+b.get_text()+']')\n",
    "                                    print(return_agr)\n",
    "                                    print('--------')\n",
    "                                    r['agr'] = return_agr\n",
    "                                    \n",
    "                                print('Applied Rule: ' + r['tag'] + '[' + str(l) + ',' + str(s) + ']' + ' --> ' + str(a.get_head()['tag']) + '[' + str(p) + ',' + str(s) + ']' + ' ' + str(b.get_head()['tag'])+ '[' + str(l-p) + ',' + str(s+p) + ']')  \n",
    "                                self.parse_table[l-1][s-1].add_production(r,a,b,a.get_text() + ' ' + b.get_text())\n",
    "                               \n",
    "        self.number_of_trees = len(self.parse_table[self.length-1][0].get_results())\n",
    "        print(\"---------------------------------------\")\n",
    "        print('Number of possible trees: ' + str(self.number_of_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "The folling example should test multiple aspects of the grammar. First, it will test the usage of a modal verb, adding complexity to basic structure. Also it will test number agreement beetween 'I' and 'fly'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Unified number/tense agreement: [IN from] [NP Alaska]\n",
      "[]\n",
      "--------\n",
      "Applied Rule: PP[2,4] --> IN[1,4] NP[1,5]\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from Alaska]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from Alaska]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "Applied Rule: VP[3,3] --> VB[1,3] PP[2,4]\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "Applied Rule: VP[4,2] --> MD[1,2] VP[3,3]\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num = 'sg' ]\n",
      "--------\n",
      "Applied Rule: S[5,1] --> NP[1,1] VP[4,2]\n",
      "---------------------------------------\n",
      "Number of possible trees: 1\n",
      "------  ------  ------  ------  ------\n",
      "['S']\n",
      "[]      ['VP']\n",
      "[]      []      ['VP']\n",
      "[]      []      []      ['PP']\n",
      "['NP']  ['MD']  ['VB']  ['IN']  ['NP']\n",
      "I       might   fly     from    Alaska\n",
      "------  ------  ------  ------  ------\n"
     ]
    }
   ],
   "source": [
    "pt = Parse_Table(cfg)\n",
    "pt.parse('I might fly from Alaska')\n",
    "pt.print_parse_table()\n",
    "trees = pt.get_trees()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 Reflection:\n",
    "\n",
    "Pary 2 was great since I felt like I was incorporating so many different parts of my undergraduate coursework into 1 algorith. It was logic, dynamic programming, and parsing concepts all packed into 1. I think being able to pass in testing sentences also really helped to evaluate the efficacy of my grammar rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Search-Based NLG\n",
    "Now, we will begin generating semantically-important text on top of the context-free grammar. The alogorithm that is most up to task is best-first search. This is because search allows you to explore multiple different paths. These paths begin in a queue and are popped so that neighboring options can be viewed. It supports the usage of heuristic functions, which evaluates which path is most promising. I used this guide to trace the algorithm: https://www.geeksforgeeks.org/best-first-search-informed-search/\n",
    "\n",
    "Initially, we will be generating ALL possible phrases given a list of entries. However, later on we will experiment with different heuristics to improve the output of the search algorithm.\n",
    "\n",
    "This particiular instance of the search function is quite simple. It begins with a single list of productions. When it finds a new way to join two nodes together, it pops the two nodes from the list, joins them and adds it back to the queue. It continues to do this, effectively creating branches of potential tree derviations.\n",
    "\n",
    "Note: check_duplicate() is intended to prevent redundancy in the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(start, grammar):\n",
    "        \n",
    "        #stores complete sentences (sentences that are the correct length and have 'S' at the head)\n",
    "        complete_sentences = []\n",
    "        \n",
    "        #queue that holds lists of productions\n",
    "        q = []\n",
    "        #first list is added before the loop\n",
    "        q.append(start)\n",
    "        \n",
    "        while len(q) > 0:\n",
    "            #item at first index is popped, the algorithm still assumes CNF and attempts to nodes that can be joined\n",
    "            curr_list = q.pop(0) \n",
    "            for i in range(len(curr_list)):\n",
    "                for j in range(len(curr_list)):\n",
    "                    \n",
    "                    #checks to make sure the node did not match with itself\n",
    "                    if curr_list[j] != curr_list[i]:\n",
    "                        \n",
    "                        if(curr_list[i].get_head() is None or curr_list[j].get_head() is None):\n",
    "                            continue\n",
    "                                                \n",
    "                        #finds the rule that joins the two nodoes\n",
    "                        if grammar.apply_rules(str(curr_list[i].get_head()['tag']) + ' ' + str(curr_list[j].get_head()['tag'])) != None:\n",
    "                            first_node = curr_list[i]\n",
    "                            first_index = i\n",
    "                            second_node = curr_list[j] \n",
    "                            second_index = j\n",
    "                        elif grammar.apply_rules(str(curr_list[j].get_head()['tag']) + ' ' + str(curr_list[i].get_head()['tag'])) != None:\n",
    "                            first_node = curr_list[j]\n",
    "                            first_index = j\n",
    "                            second_node = curr_list[i]    \n",
    "                            second_index = i\n",
    "                        else:\n",
    "                            continue\n",
    "                        \n",
    "                        temp_head = grammar.apply_rules(str(first_node.get_head()['tag']) + ' ' + str(second_node.get_head()['tag']))\n",
    "\n",
    "                        if (first_node.get_head()['tag'] == 'VP' or first_node.get_head()['tag'] == 'VB') and first_node.get_head().has_key('subcat'):\n",
    "                            return_subcat = verb_argument(first_node, second_node, grammar)\n",
    "                            if return_subcat is None:\n",
    "                                continue\n",
    "                            else:\n",
    "                                print('--------')\n",
    "                                print('Unified verb argument: ['+str(first_node.get_head()['tag'])+' '+first_node.get_text()+'] ['+str(second_node.get_head()['tag'])+' '+second_node.get_text()+']')\n",
    "                                print(return_subcat)\n",
    "                                print('--------')\n",
    "                                temp_head['subcat'] = return_subcat\n",
    "                    \n",
    "                        return_agr = unification(first_node,second_node,grammar)\n",
    "                        if return_agr is None:\n",
    "                            print(str(first_node.get_head()['agr']))\n",
    "                            print(str(second_node.get_head()['agr']))\n",
    "                            print('Error number/tense agreement: ['+str(first_node.get_head()['tag'])+' '+first_node.get_text()+'] ['+str(second_node.get_head()['tag'])+' '+second_node.get_text()+']')\n",
    "                            continue\n",
    "                        else:\n",
    "                            print('--------')\n",
    "                            print('Unified number/tense agreement: ['+str(first_node.get_head()['tag'])+' '+first_node.get_text()+'] ['+str(second_node.get_head()['tag'])+' '+second_node.get_text()+']')\n",
    "                            print(return_agr)\n",
    "                            print('--------')\n",
    "                            temp_head['agr'] = return_agr\n",
    "                        \n",
    "                        #makes a copy of the list popped from q -> this is so the list can be modified and pushed back\n",
    "                        temp_list = curr_list.copy()\n",
    "                            \n",
    "                        #to prevent the wrong item from getting popped if the list shrinks after the first\n",
    "                        if first_index < second_index:\n",
    "                            temp_node_1 = temp_list.pop(first_index)\n",
    "                            temp_node_2 = temp_list.pop(second_index-1)\n",
    "                        else:\n",
    "                            temp_node_1 = temp_list.pop(first_index)\n",
    "                            temp_node_2 = temp_list.pop(second_index)\n",
    "                                \n",
    "                        #creates a new node and modifies it to the copied list\n",
    "                        temp_string = temp_node_1.get_text() + ' ' + temp_node_2.get_text()\n",
    "                        temp_list.append(Production(temp_head, temp_node_1, temp_node_2, temp_string))\n",
    "                            \n",
    "                        #tries to prevent duplicate\n",
    "                        if temp_list not in q:\n",
    "                                \n",
    "                            #if the node meets conditions, added to complete sentences\n",
    "                            if temp_head['tag'] == 'S' and len(temp_string.split(' ')) == len(start) and not grammar.check_duplicate(complete_sentences, temp_string):\n",
    "                                complete_sentences.append(temp_list)\n",
    "                                \n",
    "                            q.append(temp_list)\n",
    "                                    \n",
    "        #prints complete sentences along with corresponding tags\n",
    "        for i in complete_sentences:\n",
    "            print(i[0].get_text())\n",
    "            print('[' + str(i[0].get_head()['tag']) + ' ' + grammar.print_expansion(i[0]) + ']')\n",
    "            print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Test the following list of inputs. All of the words are already part of the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Unified number/tense agreement: [IN from] [NP Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [IN from] [NP I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [IN from] [NP Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [IN from] [NP I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from Alaska]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from Alaska]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from Alaska]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from Alaska]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from I]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from I]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from I]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from I]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from Alaska]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from Alaska]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from Alaska]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from Alaska]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from I]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from I]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified verb argument: [VB fly] [PP from I]\n",
      "[ empty = 'y' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [VB fly] [PP from I]\n",
      "[ num   = 'pl'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from Alaska]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [MD might] [VP fly from I]\n",
      "[]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP I] [VP might fly from Alaska]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "--------\n",
      "Unified number/tense agreement: [NP Alaska] [VP might fly from I]\n",
      "[ num   = 'sg'   ]\n",
      "[ tense = 'pres' ]\n",
      "--------\n",
      "I might fly from Alaska\n",
      "[S [NP I][VP [MD might][VP [VB fly][PP [IN from][NP Alaska]]]]]\n",
      "-------------------------\n",
      "Alaska might fly from I\n",
      "[S [NP Alaska][VP [MD might][VP [VB fly][PP [IN from][NP I]]]]]\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "inputs = ['might','Alaska','I','from','fly']\n",
    "productions = []\n",
    "for i in inputs:\n",
    "    productions.append(Production(cfg.apply_rules(i),None,None,i))\n",
    "search(productions, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can analyze the unifications steps taken during search. The printed statements are the strucutres that are returned to each parent. The tagged sentences at the bottom are both valid setences according to the context free grammar. Also note that they were able to parse this sentence earlier. That signifies that our grammar works as a medium for both parsing and generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Reflection\n",
    "\n",
    "This section was useful since it really showed the round trip program. I liked seeing how everything built on top of the simple grammar class. I also enjoyed making engineering decisions to cut down on overhead in this section - namely, trying to keep duplicates out of the search queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary \n",
    "I am extremely satisfied with the result and how the notebook seemed to evolve from my original idea. I think the most interesting part is how things seemed to fit together like a puzzle. Aspects of language that previously seemed arbitrarily constructed could actually be codified into rules. More interesting than that even was how parsing and generation took very similar approaches to interpreting nodes and building out trees. I think any individual, even someone with little to no knowledge of the area, would appraoch language with a different perspective after gaining a basic understanding of these concepts.\n",
    "\n",
    "This notebook does a very good job with how language can be structured within code. There is no doubt that this code could become more and more complex with the structure. This comes from more grammar rules, edge cases, and heuristics to help tackle structural ambiguities. However, a bigger question to tackle is how to represent meaning. Is it possible to approach the idea of semantics with the same formality, or would one have to turn to a more statistical approach. This is the next think I would explore, to begin placing meaning contraints on generations, and even analyze the intersection of language and logic to solve simple problems.\n",
    "\n",
    "Additionally, one of the original goals of this paper was to make the example-based. That comes with some kind of learning mechanism, to take the patterns from above and extrapolate them to gain an understanding of new rules. There were many facets to this problem, ranging from different learning algorithms, to how to tag inputs and induce their linguistic information. However, one of my goals would still be to keep the training process as lightweight as possible. I am still very much interested in exploring a nativist appraoch to grammar, and test poverty of stimulus simulation. Therfore, the central idea to tackle would be: how to provide a rigid foundation of language so that the the system could acquire language from as little information as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
